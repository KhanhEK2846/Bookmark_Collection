# Classification

---

<h2 title="
Good:
- Can solve non-linear problems.
- Can work on high-dimensional data with excellent accuracy.
- Easy to visualize and explain.
Bad:
- Overfitting. Might be resolved by random forest. A small change in the data can lead to a large change in the structure of the optimal decision tree.
- Calculations can get very complex.
"> Decision Tree </h2>

> Coming Soon

<h2 title="
- Can make predictions without training.
- Time complexity is O(n).
- Can be used for both classification and regression.
Bad:
- Does not work well with large dataset.
- Sensitive to noisy data, missing values and outliers.
- Need feature scaling.
- Choose the correct K value.
"
> K Nearest Neighbors Algorithm (KNN) </h2>

> Coming Soon

<h2 title="
Good:
- Less prone to over-fitting but it can overfit in high dimensional datasets.
- Efficient when the dataset has features that are linearly separable.
- Easy to implement and efficient to train.
Bad:
- Should not be used when the number of observations are lesser than the number of features.
- Assumption of linearity which is rare in practise.
- Can only be used to predict discrete functions.
"> Logistic Regression </h2>

> Coming Soon

<h2 title="
- Training period is less.
- Better suited for categorical inputs.
- Easy to implement.
Bad:
- Assumes that all features are independent which is rarely happening in real life.
- Zero Frequency.
- Estimations can be wrong in some cases.
"> Naive Bayes Classifier </h2>

> Coming Soon

<h2 title="
Good:
- Good at high dimensional data.
- Can work on small dataset.
- Can solve non-linear problems.
Bad:
- Inefficient on large data.
- Requires picking the right kernal.
"> Support Vector Machine (SVM) </h2>

- [Kernel Method](./SVM/Kernel.md)

---

[<kbd> Back </kbd>](./../readme.md)
